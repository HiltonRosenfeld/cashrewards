{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Populate Vector DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Data from Web Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load a number of HTML pages using `request` module. Each of those pages contains lots of superfluous content so we extract only the relevant article context.\n",
    "\n",
    "The parsed data is saved into a Pickle file so that we don't have to crawl the website again if we need to recreate the vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\"\"\"\n",
    "Function to calculate the number of tokens in a text string.\n",
    "\"\"\"\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def num_tokens_from_string(string: str) -> int:\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def clean_text(text: str):\n",
    "    \"\"\"\n",
    "    Function to clean text from web pages\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalize line breaks to \\n\\n (two new lines)\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\\n\")\n",
    "    text = text.replace(\"\\r\", \"\\n\\n\")\n",
    "\n",
    "    # Replace two or more spaces with a single space\n",
    "    text = re.sub(\" {2,}\", \" \", text)\n",
    "\n",
    "    # Remove leading spaces before removing trailing spaces\n",
    "    text = re.sub(\"^[ \\t]+\", \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove trailing spaces before removing empty lines\n",
    "    text = re.sub(\"[ \\t]+$\", \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove empty lines\n",
    "    text = re.sub(\"^\\s+\", \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "    # remove unicode Non Breaking Space\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get list of URLs from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read urls from file named links.txt\n",
    "with open('urls_tv.txt', 'r') as file:\n",
    "    urls = file.readlines()\n",
    "    urls = [url.strip() for url in urls]\n",
    "\n",
    "# prepend \"https://www.thegoodguys.com.au/\" to each url\n",
    "urls = [\"https://www.thegoodguys.com.au\" + url for url in urls]\n",
    "\n",
    "# For debugging, override the list and use only a single URL\n",
    "#urls = [\"https://www.thegoodguys.com.au/lg-50-inches-ut8050-4k-uhd-led-smart-tv-24-50ut8050psb\",\n",
    "#        \"https://www.thegoodguys.com.au/hisense-100-inches-q7nau-4k-qled-smart-tv-24-100q7nau\",\n",
    "#        \"https://www.thegoodguys.com.au/apple-watch-se-gps-40mm-starlight-aluminium-case-with-starlight-sport-band-sm-mr9u3zpa\"]\n",
    "\n",
    "print (f\"Number of URLs: {len(urls)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crawl URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through the URLs and create a LangChain Document object for each page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from website_parser import website_parser\n",
    "from langchain.docstore.document import Document\n",
    "import pickle\n",
    "\n",
    "data = []\n",
    "for url in urls:\n",
    "\n",
    "    # Parse website using Beautiful Soup.\n",
    "    item = website_parser(url)\n",
    "\n",
    "    url = url.replace(\"https://www.thegoodguys.com.au\", \"\")\n",
    "    \n",
    "    metadata = {\n",
    "        'source': url,\n",
    "        'title': item['title'],\n",
    "        'price': item['price'],\n",
    "        'img': item['img'],\n",
    "        'features': item['key_features'],\n",
    "        'specs': item['tech_specs'],\n",
    "        'product_features': item['product_features'],\n",
    "    }\n",
    "\n",
    "    document = Document(page_content=item['description'], metadata=metadata)\n",
    "    #data.append(document)\n",
    "    \n",
    "    with open(f\"pickles/{url}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(document, f)\n",
    "\n",
    "print (f\"Number of Documents: {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write Crawled Data to Disk**\n",
    "\n",
    "*WARNING: Only run this block if you want to recreate the Pickle file*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#write data to file in a way that it can be reconstituted into a list of documents\n",
    "with open(\"website_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read Crawled Data from Disk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read help_data.pkl and recreate data object as list of documents\n",
    "import pickle\n",
    "with open(\"website_data.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Pickle Files in pickles directory and recreate data object as list of documents\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "data = []\n",
    "for file in os.listdir(\"pickles\"):\n",
    "    if file.endswith(\".pkl\"):\n",
    "        with open(f\"pickles/{file}\", \"rb\") as f:\n",
    "            document = pickle.load(f)\n",
    "            data.append(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformat Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to try different embeding apporaches to see what works best:\n",
    "- Text Embedding\n",
    "    - Description only\n",
    "    - Concatenate description, features, specs, price\n",
    "- Knowledge Graph\n",
    "    - Description only\n",
    "    - Concatenate description, features, specs, price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate consumer description\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "def generate_consumer_description(product):\n",
    "    # Create Prompt\n",
    "    message_objects = []\n",
    "    message_objects.append({\"role\":\"user\",\n",
    "     \"content\": f\"Provide a single paragraph consumer level description of the product: {product}\"})\n",
    "    \n",
    "    completion = client.chat.completions.create(model=\"gpt-4o\",messages=message_objects)\n",
    "    #completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",messages=message_objects)\n",
    "    consumer_description = completion.choices[0].message.content\n",
    "\n",
    "    return consumer_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "data_concatenated = []\n",
    "for document in data:\n",
    "    # Generate consumer description\n",
    "    consumer_description = generate_consumer_description(document.metadata['title'])\n",
    "\n",
    "    #content  = f\"Description:\\n{document.page_content}\\n\\n\"\n",
    "    content  = f\"{document.page_content}\\n\\n\"\n",
    "    content += f\"{consumer_description}\\n\\n\"\n",
    "    #content += f\"Title:\\n{document.metadata['title']}\\n\\n\"\n",
    "    #content += f\"Key Features:\\n{document.metadata['features']}\\n\\n\"\n",
    "    #content += f\"{document.metadata['features']}\\n\\n\"\n",
    "    #content += f\"Technical Specifications:\\n{document.metadata['specs']}\"\n",
    "    #content += f\"Product Features:\\n{document.metadata['product_features']}\"\n",
    "    content = clean_text(content)\n",
    "    #print(content)\n",
    "    \n",
    "    source = document.metadata['source']\n",
    "    source = source.replace(\"https://www.thegoodguys.com.au\", \"\")\n",
    "\n",
    "    brand = document.metadata['title']\n",
    "    # split brand using delimet of nbsp &nbsp\n",
    "    brand = brand.split(\"\\xa0\")[0]\n",
    "    #brand = brand.split(\" \")[0]\n",
    "\n",
    "    metadata = {\n",
    "        'source': source,\n",
    "        'brand': clean_text(brand),\n",
    "        'title': clean_text(document.metadata['title']),\n",
    "        'price': document.metadata['price'],\n",
    "        'img': document.metadata['img'],\n",
    "    }\n",
    "\n",
    "    document_concatenated = Document(page_content=content, metadata=metadata)\n",
    "    data_concatenated.append(document_concatenated)\n",
    "\n",
    "print(len(data_concatenated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove Non Television Products**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = []\n",
    "for d in data_concatenated:\n",
    "    title = d.metadata['title']\n",
    "    # if title contains \"antenna\", or \"mount\" then skip\n",
    "    if \"antenna\" in title.lower() or \"mount\" in title.lower() or \"wall bracket\" in title.lower() or \"stand\" in title.lower() or \"bracket\" in title.lower():\n",
    "        continue\n",
    "    if \"tv\" not in title.lower():\n",
    "        continue\n",
    "    data_new.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Data\n",
      "Number of chunks: 385\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "CHUNK_SIZE = 1500\n",
    "\n",
    "# Chunk the data\n",
    "print(\"Splitting Data\")\n",
    "text_splitter = TokenTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=100)\n",
    "docs = text_splitter.split_documents(data_new)\n",
    "print(f\"Number of chunks: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AstraDB Connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "ASTRA_DB_API_ENDPOINT = os.environ.get(\"ASTRA_DB_API_ENDPOINT\")\n",
    "ASTRA_DB_APPLICATION_TOKEN = os.environ.get(\"ASTRA_DB_APPLICATION_TOKEN\")\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "ASTRA_DB_KEYSPACE = \"cashrewards\"\n",
    "ASTRA_DB_COLLECTION = \"goodguys_ai_description\"\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Vector Store: cashrewards - goodguys_ai_description\n"
     ]
    }
   ],
   "source": [
    "#from langchain_community.vectorstores.astradb import AstraDB\n",
    "from langchain_astradb import AstraDBVectorStore\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Set up the vector store\n",
    "print(f\"Setup Vector Store: {ASTRA_DB_KEYSPACE} - {ASTRA_DB_COLLECTION}\")\n",
    "vectorstore = AstraDBVectorStore(\n",
    "    embedding=embeddings,\n",
    "    namespace=ASTRA_DB_KEYSPACE,\n",
    "    collection_name=ASTRA_DB_COLLECTION,\n",
    "    token=ASTRA_DB_APPLICATION_TOKEN,\n",
    "    api_endpoint=ASTRA_DB_API_ENDPOINT,\n",
    "    #metric=\"dot_product\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store data and embeddings in Astra DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding texts to Vector Store\n",
      "Adding 0 to 50 Inserted 50 documents.\n",
      "Adding 50 to 100 Inserted 50 documents.\n",
      "Adding 100 to 150 Inserted 50 documents.\n",
      "Adding 150 to 200 Inserted 50 documents.\n",
      "Adding 200 to 250 Inserted 50 documents.\n",
      "Adding 250 to 300 Inserted 50 documents.\n",
      "Adding 300 to 350 Inserted 50 documents.\n",
      "Adding 350 to 400 Inserted 35 documents.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"Adding texts to Vector Store\")\n",
    "\n",
    "BLOCK_SIZE = 50\n",
    "# iterate through docs in sets of BLOCK_SIZE\n",
    "for i in range(0, len(docs), BLOCK_SIZE):\n",
    "    print(f\"Adding {i} to {i+BLOCK_SIZE}\", end=' ')\n",
    "    texts, metadatas = zip(*((doc.page_content, doc.metadata) for doc in docs[i:i+BLOCK_SIZE]))\n",
    "    inserted_ids = vectorstore.add_texts(texts=texts, metadatas=metadatas)\n",
    "    print(f\"Inserted {len(inserted_ids)} documents.\")\n",
    "    # pause for 1 seconds\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete all Documents in Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*WARNING*: This code will delete all documents from the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astrapy.db import AstraDB\n",
    "\n",
    "# Initialize the AstraDB client\n",
    "db = AstraDB(\n",
    "    namespace=ASTRA_DB_KEYSPACE,\n",
    "    token=ASTRA_DB_APPLICATION_TOKEN,\n",
    "    api_endpoint=ASTRA_DB_API_ENDPOINT,\n",
    ")\n",
    "\n",
    "\n",
    "# Retrieve collections\n",
    "collections_response = db.get_collections()\n",
    "\n",
    "# validate that ASTRA_DB_COLLECTION exists in collections_response[\"status\"][\"collections\"]\n",
    "if ASTRA_DB_COLLECTION in collections_response[\"status\"][\"collections\"]:\n",
    "    print(f\"Collection \\\"{ASTRA_DB_COLLECTION}\\\" exists\")\n",
    "\n",
    "    # Access an existing collection\n",
    "    collection = db.collection(ASTRA_DB_COLLECTION)\n",
    "\n",
    "    # Delete all documents in the collection\n",
    "    res = collection.delete_many(filter={})\n",
    "\n",
    "    # Print the result\n",
    "    print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_vector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
